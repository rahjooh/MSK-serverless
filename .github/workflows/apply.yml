name: Terraform Apply

on:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

concurrency:
  group: terraform-${{ github.ref }}
  cancel-in-progress: false

env:
  TERRAFORM_VERSION: 1.6.6
  TF_WORKING_DIR: .
  PLAN_FILE: plan.out
  TF_VARS_FILE: ci.auto.tfvars
  TF_BACKEND_FILE: backend.auto.tfbackend
  AWS_REGION: ap-south-1
  INFRA_SUMMARY_BUCKET: test-raw-databucket
  INFRA_SUMMARY_KEY: terraform/cluster/resources.json
  USE_EXISTING: 'true'
  S3_TFVARS_BUCKET: ${{ secrets.S3_TFVARS_BUCKET || vars.S3_TFVARS_BUCKET || '' }}
  S3_TFVARS_KEY: ${{ secrets.S3_TFVARS_KEY || vars.S3_TFVARS_KEY || '' }}

jobs:
  apply:
    name: Apply infrastructure
    runs-on: ubuntu-latest
    environment:
      name: production
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Resolve runtime configuration
        run: |
          {
            echo "ASSUME_ROLE_ARN<<EOF";
            echo "${ASSUME_ROLE_ARN}";
            echo "EOF";
            echo "AWS_REGION_EFFECTIVE<<EOF";
            echo "${AWS_REGION_INPUT}";
            echo "EOF";
            echo "TFVARS_CONTENT<<EOF";
            echo "${TFVARS}";
            echo "EOF";
            echo "TF_BACKEND_CONTENT<<EOF";
            echo "${TF_BACKEND_CONFIG}";
            echo "EOF";
            echo "AWS_ACCESS_KEY_ID<<EOF";
            echo "${AWS_ACCESS_KEY_ID_INPUT}";
            echo "EOF";
            echo "AWS_SECRET_ACCESS_KEY<<EOF";
            echo "${AWS_SECRET_ACCESS_KEY_INPUT}";
            echo "EOF";
            echo "AWS_SESSION_TOKEN<<EOF";
            echo "${AWS_SESSION_TOKEN_INPUT}";
            echo "EOF";
            echo "AWS_DEFAULT_REGION<<EOF";
            echo "${AWS_REGION_INPUT}";
            echo "EOF";
          } >> "$GITHUB_ENV"
        env:
          ASSUME_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN || vars.AWS_ROLE_ARN || '' }}
          AWS_REGION_INPUT: ${{ secrets.AWS_REGION || vars.AWS_REGION || env.AWS_REGION }}
          TFVARS: ${{ secrets.TERRAFORM_TFVARS || '' }}
          TF_BACKEND_CONFIG: ${{ secrets.TF_BACKEND_CONFIG || vars.TF_BACKEND_CONFIG || '' }}
          AWS_ACCESS_KEY_ID_INPUT: ${{ secrets.AWS_ACCESS_KEY_ID || '' }}
          AWS_SECRET_ACCESS_KEY_INPUT: ${{ secrets.AWS_SECRET_ACCESS_KEY || '' }}
          AWS_SESSION_TOKEN_INPUT: ${{ secrets.AWS_SESSION_TOKEN || '' }}

      - name: Avoid double assume (clear TF_VAR_assume_role_arn when GH already assumed)
        if: ${{ env.ASSUME_ROLE_ARN != '' }}
        run: |
          {
            echo "TF_VAR_assume_role_arn<<EOF";
            echo "";
            echo "EOF";
          } >> "$GITHUB_ENV"

      - name: Materialize tfvars from secret
        if: ${{ env.TFVARS_CONTENT != '' }}
        run: |
          printf '%s\n' "${TFVARS_CONTENT}" > "${TF_VARS_FILE}"
          terraform fmt "${TF_VARS_FILE}"
        env:
          TFVARS_CONTENT: ${{ env.TFVARS_CONTENT }}
          TF_VARS_FILE: ${{ env.TF_VARS_FILE }}

      - name: Materialize backend config from secrets/vars
        if: ${{ env.TF_BACKEND_CONTENT != '' }}
        run: |
          printf '%s\n' "${TF_BACKEND_CONTENT}" > "${TF_BACKEND_FILE}"
        env:
          TF_BACKEND_CONTENT: ${{ env.TF_BACKEND_CONTENT }}
          TF_BACKEND_FILE: ${{ env.TF_BACKEND_FILE }}

      - name: Validate backend config (skip invalid)
        if: ${{ hashFiles(env.TF_BACKEND_FILE) != '' }}
        run: |
          set -euo pipefail
          FILE="${TF_BACKEND_FILE}"
          if ! grep -Eq '^[[:space:]]*(bucket|key)[[:space:]]*=' "$FILE"; then
            echo "::notice::Invalid backend config in $FILE; removing so init can use default/backend args."
            rm -f "$FILE"
          fi

      - name: Read backend from versions.tf (fallback)
        if: ${{ hashFiles(env.TF_BACKEND_FILE) == '' }}
        run: |
          python3 - <<'PY'
          import os, re, sys
          workdir = os.environ.get('TF_WORKING_DIR', '.') or '.'
          path = os.path.join(workdir, 'versions.tf')
          if not os.path.isfile(path):
              sys.exit(0)
          content = open(path, 'r', encoding='utf-8').read()
          m = re.search(r'backend\s+"s3"\s*\{', content)
          if not m:
              sys.exit(0)
          i = m.end()
          depth = 1
          kv = {}
          while i < len(content) and depth > 0:
              ch = content[i]
              if ch == '{':
                  depth += 1
              elif ch == '}':
                  depth -= 1
              i += 1
              if depth == 0:
                  break
          block = content[m.end():i-1]
          for line in block.splitlines():
              line = line.split('#',1)[0].strip()
              if not line or '=' not in line:
                  continue
              k, v = line.split('=', 1)
              k = k.strip()
              v = v.strip().strip('"')
              if k in {'bucket','key','region','dynamodb_table'}:
                  kv[k] = v
          env_path = os.environ.get('GITHUB_ENV')
          if env_path:
              with open(env_path, 'a', encoding='utf-8') as fh:
                  if 'bucket' in kv:
                      fh.write(f"TF_BACKEND_BUCKET={kv['bucket']}\n")
                  if 'key' in kv:
                      fh.write(f"TF_BACKEND_KEY={kv['key']}\n")
                  if 'region' in kv:
                      fh.write(f"TF_BACKEND_REGION={kv['region']}\n")
                  if 'dynamodb_table' in kv:
                      fh.write(f"TF_BACKEND_DYNAMODB_TABLE={kv['dynamodb_table']}\n")
          PY

      - name: Configure AWS credentials (assume role)
        if: ${{ env.ASSUME_ROLE_ARN != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.ASSUME_ROLE_ARN }}
          aws-region: ${{ env.TF_BACKEND_REGION || env.AWS_REGION_EFFECTIVE || env.AWS_REGION }}
          role-duration-seconds: 3600
          role-skip-session-tagging: true
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ env.AWS_SESSION_TOKEN }}

      - name: Configure AWS credentials (static)
        if: ${{ env.ASSUME_ROLE_ARN == '' && env.AWS_ACCESS_KEY_ID != '' && env.AWS_SECRET_ACCESS_KEY != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ env.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.TF_BACKEND_REGION || env.AWS_REGION_EFFECTIVE || env.AWS_REGION }}

      - name: Validate AWS credential configuration
        if: ${{ env.ASSUME_ROLE_ARN == '' && (env.AWS_ACCESS_KEY_ID == '' || env.AWS_SECRET_ACCESS_KEY == '') }}
        run: |
          echo "::error::AWS credentials are not configured. Provide ASSUME_ROLE_ARN (optionally with access keys for source credentials) or supply AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY."
          exit 1

      - name: Detect backend bucket region
        if: ${{ env.TF_BACKEND_BUCKET != '' || hashFiles(env.TF_BACKEND_FILE) != '' }}
        run: |
          set -euo pipefail
          BUCKET="${TF_BACKEND_BUCKET:-}"
          if [ -z "${BUCKET}" ] && [ -s "${TF_BACKEND_FILE}" ]; then
            BUCKET=$(awk -F'=' '/^[[:space:]]*bucket[[:space:]]*=/{gsub(/[#].*$/,""); v=$2; gsub(/^[[:space:]]+|[[:space:]]+$/,"",v); gsub(/"/ , "", v); print v}' "${TF_BACKEND_FILE}" || true)
          fi
          if [ -n "${BUCKET}" ]; then
            REGION=""
            # Try to detect the bucket's actual region. Only override when detection succeeds.
            if REGION_RAW=$(aws s3api get-bucket-location --bucket "${BUCKET}" --query 'LocationConstraint' --output text 2>/dev/null); then
              REGION="$REGION_RAW"
              # AWS returns "None" specifically for us-east-1
              if [ "${REGION}" = "None" ] || [ -z "${REGION}" ]; then REGION="${AWS_REGION_EFFECTIVE:-${AWS_REGION:-us-east-1}}"; fi
              echo "TF_BACKEND_REGION=${REGION}" >> "$GITHUB_ENV"
            else
              echo "::notice::Could not determine S3 bucket region for '${BUCKET}'. Backend region will not be overridden." >&2
            fi
            echo "TF_BACKEND_BUCKET=${BUCKET}" >> "$GITHUB_ENV"
          fi

      - name: Download terraform.tfvars from S3 (optional)
        if: ${{ env.S3_TFVARS_BUCKET != '' && env.S3_TFVARS_KEY != '' }}
        run: |
          aws s3 cp "s3://${S3_TFVARS_BUCKET}/${S3_TFVARS_KEY}" terraform.tfvars
          # also mirror to auto tfvars so Terraform loads it implicitly
          cp terraform.tfvars "${TF_VARS_FILE}" || true
          terraform fmt terraform.tfvars || true

      - name: Terraform init (via Makefile)
        run: |
          REGION="${TF_BACKEND_REGION:-${AWS_REGION_EFFECTIVE:-${AWS_REGION:-}}}"
          if [ -n "${TF_BACKEND_BUCKET:-}" ] && [ -n "${TF_BACKEND_KEY:-}" ]; then \
            ARGS="-backend-config=bucket=${TF_BACKEND_BUCKET} -backend-config=key=${TF_BACKEND_KEY}"; \
            if [ -n "${REGION}" ]; then ARGS="$ARGS -backend-config=region=${REGION}"; fi; \
            if [ -n "${TF_BACKEND_DYNAMODB_TABLE:-}" ]; then ARGS="$ARGS -backend-config=dynamodb_table=${TF_BACKEND_DYNAMODB_TABLE}"; fi; \
            make INIT_ARGS="$ARGS" init; \
          elif [ -s "${TF_BACKEND_FILE}" ]; then \
            if [ -n "${TF_BACKEND_REGION}" ]; then \
              make INIT_ARGS="-backend-config=${TF_BACKEND_FILE} -backend-config=region=${REGION}" init; \
            else \
              make INIT_ARGS="-backend-config=${TF_BACKEND_FILE}" init; \
            fi; \
          else \
            make init; \
          fi

      - name: Export VPC_ID/AWS_REGION from tfvars (if present)
        run: |
          python <<'PY'
          import ast, os, sys
          candidates = ['terraform.tfvars', os.environ.get('TF_VARS_FILE','')]
          out = {}
          def scan(path):
              if not path or not os.path.isfile(path):
                  return False
              with open(path, 'r', encoding='utf-8') as fh:
                  for raw in fh:
                      if '=' not in raw:
                          continue
                      k, v = raw.split('=', 1)
                      k = k.strip()
                      if k not in {'vpc_id','region'}:
                          continue
                      v = v.split('#',1)[0].strip()
                      try:
                          out[k] = ast.literal_eval(v)
                      except Exception:
                          out[k] = v.strip('"')
              return True
          for c in candidates:
              if scan(c):
                  break
          env_path = os.environ.get('GITHUB_ENV')
          if env_path:
              with open(env_path, 'a', encoding='utf-8') as fh:
                  if 'vpc_id' in out:
                      fh.write(f"VPC_ID={out['vpc_id']}\n")
                  if 'region' in out:
                      fh.write(f"AWS_REGION={out['region']}\n")
          PY

      - name: Show existing resources (pre-apply)
        if: ${{ env.USE_EXISTING == 'true' }}
        run: make check-existing

      - name: Terraform apply (use existing if found)
        if: ${{ env.USE_EXISTING == 'true' }}
        run: make apply-use-existing

      - name: Terraform apply (fresh)
        if: ${{ env.USE_EXISTING != 'true' }}
        run: make apply

      - name: Save infrastructure summary
        run: terraform -chdir=${{ env.TF_WORKING_DIR }} output -json infrastructure_summary_json > resources.json

      - name: Upload infrastructure summary
        uses: actions/upload-artifact@v4
        with:
          name: infra-summary
          path: resources.json

      - name: Publish summary to S3
        if: ${{ env.INFRA_SUMMARY_BUCKET != '' && env.INFRA_SUMMARY_KEY != '' }}
        run: aws s3 cp resources.json "s3://${{ env.INFRA_SUMMARY_BUCKET }}/${{ env.INFRA_SUMMARY_KEY }}"

      - name: Capture outputs
        run: |
          terraform -chdir=${{ env.TF_WORKING_DIR }} output -json > terraform-outputs.json
          terraform -chdir=${{ env.TF_WORKING_DIR }} output > terraform-outputs.txt

      - name: Upload outputs artifact
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: |
            terraform-outputs.json
            terraform-outputs.txt

  report:
    name: Report created infrastructure
    runs-on: ubuntu-latest
    needs: apply
    if: ${{ needs.apply.result == 'success' }}
    steps:
      - name: Download outputs artifact
        uses: actions/download-artifact@v4
        with:
          name: terraform-outputs
          path: .

      - name: Publish outputs to summary
        run: |
          {
            echo '### Terraform Outputs';
            echo '';
            cat terraform-outputs.txt;
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Upload outputs as job artifact
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs-report
          path: |
            terraform-outputs.json
            terraform-outputs.txt
